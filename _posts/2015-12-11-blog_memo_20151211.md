---
layout: post
title:  "기술 관련 뉴스 및 정보들  (2015/12/11)"
date:   2015-12-11 00:13:00
tags: [Science & Technology]
---

## 뉴스기사 

### Facebook 에서 딥러닝 학습을 위한 big sur 라는 하드웨어 플랫폼 구조를 오픈하였다.
[링크1](http://www.wired.com/2015/12/facebook-open-source-ai-big-sur/)
[링크2](https://code.facebook.com/posts/1687861518126048/facebook-to-open-source-ai-hardware-design/)
근데 내부적으로는 별 것이 없어 보이는데, chip design 을 공개한 것도 아니고 그냥 GPU 여러개를 장착하고 쿨링을 좀 효율적으로 하는 등의 내용이라서 오픈 하드웨어 플랫폼이라고 보기엔 좀 별 것 아니어 보이기도 한다. HN 코맨트에는 그런 점들을 지적하는 내용들이 많다.

### 어제 IPO한 atlassian 의 주식이 올라서 $6B 가까이 기업가치를 평가받게 되었다.
[기사링크](http://www.bloomberg.com/news/articles/2015-12-10/atlassian-climbs-in-trading-debut-after-pricing-ipo-above-range)
기사를 읽어보면 대략적인 창업스토리를 알 수 있는데, 여러 모로 대단하며, 참고할 만한 내용이다. 물론 우려스러운 점들이 있다. 대부분 제품이 개발자를 위한 것들인데, 개발자들은 새롭고 더 좋은 제품이 나오면 주저없이 갈아타기 때문에 profit sustainability 면에서 걱정이 된다는 점 등을 참고할 만 하다.

### 워싱턴 포스트가 https 로 옮겼다는 내용.
[기사링크](https://developer.washingtonpost.com/pb/blog/post/2015/12/10/moving-the-washington-post-to-https/)
사실상 내용은 별 것 없지만, 언론사가 이렇게 엔지니어링 블로그도 공개하고 좋은 툴이나 팁도 공개하며 기술적으로 뛰어난 것을 보여주는 문화적 풍토가 굉장히 부럽다.

### 독일 막스플랑크 연구소에서 헬륨 플라즈마를 만들어내는 데 성공했다고 한다.
[링크](http://www.ipp.mpg.de/3984226/12_15)
핵융합 발전에 있어서 플라즈마를 유지시키는 것이 굉장히 중요하다고 하는데, 그게 어느정도 진전이 있는 것으로 보인다. 
Wendelstein 7-X stellarator 을 십년 가까이 연구하고 제작하여 Confined by magnetic fields 방식으로 성공하였는데, 기존에 많이 사용되던 토카막과 더불어 다른 한 가지 방식인 stellarator 를 이용한 것이라고 한다.



## 정보

### SigOpt 는 Optimization만 전문으로 하는 회사이다. 기존의 Kaggle master 들이 세운 DataRobot이나 국내의 넘버웍스 처럼 데이터 분석 및 최적화와 관련된 서비스를 제공하는데, 다만, 컨설팅 보다는 일정 금액을 받고 최적화 툴 등을 제공하는 비즈니스 모델이 더 큰 것으로 보인다. 그리고 SigOpt 블로그글을 읽어보면 spearmint 처럼 bayesian optimization 을 이용한 서비스를 제공하는 것 같다. (spearmint 작성자들은 whetlab이라는 스타트업을 세운 후 트위터에 인수당했다.)
또한 이 회사의 블로그에는 좋은 내용이 굉장히 많다.
[블로그 링크](http://blog.sigopt.com/)
참고로 Gaussian Process를 위한 likelihood 구하는 내용도 읽어 볼 만 하다.
[링크](http://blog.sigopt.com/post/134931842613/sigopt-fundamentals-likelihood-for-gaussian)

### Owen Zhang 이 ODSC 2015 에서 발표한 내용을 요약한 것. 정말 좋은 내용들이 많다.
[링크](http://ninjasoul.github.io/owen-ds/)
VW 에 대한 정보도 좋고, XGBoost 파라미터 튜닝도 아주 도움되는 내용이며, categorical feature가 아주 많을 땐 LibFFM 을 사용하여 matrix factorization 기반의 방법을 쓰는 것도 굉장히 도움되는 내용이다. 또한 Neural Network 기반 라이브러리는 Theano 같은 것 보다 Keras를 추천하고 있다. Keras 가 Theano 나 Tensorflow를 backend로 사용하지만 사용법이 더 간단해서 빠른 실험을 할 수 있기 때문인 것으로 보인다. randomness나 ensemble 에 대한 내용도 도움이 많이 된다.

### xgboost 사용을 위해 categorical feature를 numeric 으로 변경하는 내용에 대한 글이다.
[링크](https://cran.r-project.org/web/packages/xgboost/vignettes/discoverYourData.html)
보다 정확히는 categorical 을 포함한 dense 데이터를 numeric sparse로 변경하는 것이다. 당연한 내용이긴 한데, random forest 와의 비교, feature importance 측정 등에 관한 내용이나 correlated feature 에 관한 내용 등이 괜찮다. categorical feature는 당연히 numeric으로 변경해야 하지만, 그 외에 numeric 인 것을 억지로 discrete으로 변경하거나 하지 않는 것이 좋다는 내용도 있다. kaggle 우승 인터뷰를 보면 많은 사람들이 feature engineering과 model 연구에 반반씩 시간을 투자한 만큼, 특징 준비하는 것이 중요한 역할을 한다.

### Dato에서도 kaggle bike sharing competition 에 xgboost 적용하는 글을 포스팅했다. (2014년도에)
[링크](http://blog.dato.com/using-gradient-boosted-trees-to-predict-bike-sharing-demand)
프로그래밍 관련 내용은 거의 GraphLab Create API 사용법이라고 할 수 있지만 접근방법은 참고할 만 하다. 참고로 xgboost 의 author도 여기에 있으며 해당 코드가 fork 되어 있다고 한다. [Dato Gallery](https://dato.com/learn/gallery/) 에 있는 글들을 보면 deep learning 관련 라이브러리도 이미 되어 있는 걸로 보인다. 시각화도 뛰어나고 여러모로 좋아 보이지만, 유료라는 점이 좀 아쉽다.

### python 을 이용하여 gradient 를 자동으로 계산해주는 라이브러리가 있다.
[링크](https://github.com/HIPS/autograd)
이미 대부분의 딥러닝 프레임워크는 이 기능을 포함하고 있지만, 참고할 만 하다.

### ICCV 2015 의 가장 인기있는 논문들을 모아놓고 간단하게 설명 해 놓았다.
[링크](http://www.computervisionblog.com/2015/12/iccv-2015-twenty-one-hottest-research.html)
유명한 컴퓨터 비전 쪽 블로거가 해 놓은 

